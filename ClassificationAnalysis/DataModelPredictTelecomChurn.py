import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn import metrics

######################## Read the tranining data ########################
#Read the csv file
telecom_df = pd.read_csv('telecom_churn.csv')
'''
print(telecom_df.head())
print(telecom_df.info())
print(telecom_df['total_day_minutes'].min())
print(telecom_df['total_day_minutes'].max())
print(telecom_df['total_day_minutes'].mean())

######################## Perform EDA and Visualization ########################

#histogram
telecom_df.hist(bins=30, figsize=(30,30), color = 'r')
plt.show()

#pie chart
telecom_df["class"].value_counts()
plt.figure(figsize = [10, 10])
telecom_df["class"].value_counts().plot(kind='pie')
plt.show()

# Correlation Matrix
corr_matrix = telecom_df.corr()
plt.figure(figsize = (15, 15))
cm = sns.heatmap(corr_matrix,
               linewidths = 1,
               annot = True, 
               fmt = ".2f")
plt.title("Correlation Matrix of Telecom Customers", fontsize = 20)
plt.show()

# Churn by day charges
#KDE = Kernel Density Estimate
ax = sns.kdeplot(telecom_df.total_day_charge[(telecom_df["class"] == 0)],
               color = "Red", fill = True)
ax = sns.kdeplot(telecom_df.total_day_charge[(telecom_df["class"] == 1)],
               color = "Blue", fill = True)

ax.legend(["Retain", "Churn"], loc = "upper right")
ax.set_ylabel("Density")
ax.set_xlabel("Day Charges")
ax.set_title("Distribution of day charges by churn")
plt.show()

#churn by evening charges
ax = sns.kdeplot(telecom_df.total_eve_charge[(telecom_df["class"] == 0)],
               color = "Red", fill = True)
ax = sns.kdeplot(telecom_df.total_eve_charge[(telecom_df["class"] == 1)],
               color = "Blue", fill = True)

ax.legend(["Retain", "Churn"], loc = "upper right")
ax.set_ylabel("Density")
ax.set_xlabel("Evening Charges")
ax.set_title("Distribution of evening charges by churn")
plt.show()

'''
######################## Prepare Data before model training ########################
#print(telecom_df.isnull().sum())
X = telecom_df.drop(["class", "area_code", "phone_number"], axis = "columns")
y = telecom_df[["class"]]

print(X.shape)
print(y.shape)

########################Perform train/test split########################
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 150)
print(X_train.shape)
print(X_test.shape)

################################################
#Below scenario is to use RandomForestClassifier to understand feature importance
'''
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train.values.ravel())
# Plot the feature importance

feat_scores= pd.DataFrame({"Fraction of variables affected" : rf.feature_importances_},index = X.columns)
feat_scores= feat_scores.sort_values(by = "Fraction of variables affected")
feat_scores.plot(kind = "barh", figsize = (10, 5))
sns.despine()
plt.show()
# The above graph is generated by Random Forest algorithm 
# The graph indicates that "total_day_minutes" tops the list of important features followed by "total_day_minutes" and so on.
'''

################################################################################################
#Train and evaluate a logistic regression classifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

model_LR = LogisticRegression(max_iter=3000)
model_LR.fit(X_train, y_train.values.ravel())

y_predict = model_LR.predict(X_test)
print("Prediction from Logistic Regression classifier \n")
print(classification_report(y_test, y_predict))
# precision is the ratio of TP/(TP+FP)
# recall is the ratio of TP/(TP+FN)
# F-beta score can be interpreted as a weighted harmonic mean of the precision and recall
# where an F-beta score reaches its best value at 1 and worst score at 0. 

#plot the confusion matrix
#cm = confusion_matrix(y_test, y_predict)
#sns.heatmap(cm, annot=True, fmt=".4g")
#plt.show()

################################################################################################
#Train and Evaluate a Support Vector Machine
from sklearn.calibration import CalibratedClassifierCV
from sklearn.svm import LinearSVC

#Below portion of code is to suppress Warning during training of the SVM model
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)
simplefilter("ignore", category=FutureWarning)

model_svc = LinearSVC(max_iter=100000)
model_svm = CalibratedClassifierCV(model_svc)
model_svm.fit(X_train,y_train.values.ravel())

y_predict = model_svm.predict(X_test)

print("Prediction from Support Vector Machine classifier \n")
print(classification_report(y_test, y_predict))

#plot the confusion matrix
#cm = confusion_matrix(y_test, y_predict)
#sns.heatmap(cm, annot=True, fmt=".4g")
#plt.show()

################################################################################################
#Train and Evaluate a Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train.values.ravel())

y_predict = rf.predict(X_test)
print("Prediction from Random Forest classifier \n")
print(classification_report(y_test, y_predict))

#plot the confusion matrix
#cm = confusion_matrix(y_test, y_predict)
#sns.heatmap(cm, annot=True, fmt=".4g")
#plt.show()

################################################################################################
#Train and Evaluate K-Nearest Neighbour classifier

from sklearn.neighbors import KNeighborsClassifier

model_knn =  KNeighborsClassifier()
model_knn.fit(X_train, y_train.values.ravel())

y_predict = model_knn.predict(X_test)
print("Prediction from KNN classifier \n")
print(classification_report(y_test, y_predict))

#plot the confusion matrix
#cm = confusion_matrix(y_test, y_predict)
#sns.heatmap(cm, annot=True, fmt=".4g")
#plt.show()

################################################################################################
#Train and Evaluate Naive Bayes classifier
from sklearn.naive_bayes import GaussianNB

model_gnb = GaussianNB()
model_gnb.fit(X_train, y_train.values.ravel())

y_predict = model_gnb.predict(X_test)
print("Prediction from Naive Bayes classifier \n")
print(classification_report(y_test, y_predict))

#plot the confusion matrix
#cm = confusion_matrix(y_test, y_predict)
#sns.heatmap(cm, annot=True, fmt=".4g")
#plt.show()


################################################################################################
#Plot ROC curve and evaluate AUC for all the 5 classifier models from above